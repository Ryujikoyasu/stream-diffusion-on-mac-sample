"""
StreamDiffusion Web Camera - Object-Oriented Refactored Version
éŸ³éŸ»ãƒˆãƒªã‚¬ãƒ¼ã«ã‚ˆã‚‹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
"""

import time
import threading
import socket
import json
import random
import os
from abc import ABC, abstractmethod
from typing import Any, Callable, List, Optional, Tuple
from queue import Queue
from dataclasses import dataclass
from datetime import datetime

import cv2
import numpy as np
from PIL import Image, ImageDraw, ImageFont
from dotenv import load_dotenv

from ..stream_diffusion import StreamDiffusion
from ..phoneme_dictionary import GOJUON_WORDS

# è¨­å®šã‚¯ãƒ©ã‚¹
@dataclass
class AppConfig:
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š"""
    sd_side_length: int = 512
    output_image_file: str = "output.png"
    tcp_host: str = "127.0.0.1"
    tcp_port: int = 65432
    gallery_dir: str = "gallery"
    max_history: int = 10
    voice_recognition_enabled: bool = True
    
    # éŸ³éŸ»ã‚·ã‚¹ãƒ†ãƒ è¨­å®š
    phoneme_max_prompts: int = 6
    phoneme_update_interval: int = 12  # 12ç§’é–“éš”
    max_selected_words: int = 3
    
    # StreamDiffusionæœ€é©åŒ–è¨­å®šï¼ˆrefç‰ˆçµ±åˆï¼‰
    optimize_for_speed: bool = True  # éŸ³éŸ»ã‚·ã‚¹ãƒ†ãƒ ã§ã¯é«˜é€ŸåŒ–ã‚’å„ªå…ˆ
    use_kohaku_model: bool = True    # èŠ¸è¡“ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
    local_cache_dir: str = "./models"

# éŸ³éŸ»ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚¯ãƒ©ã‚¹
class PhonemeProcessor:
    """éŸ³éŸ»å‡¦ç†ã¨è¾æ›¸ç®¡ç†"""
    
    def __init__(self, config: AppConfig):
        self.config = config
        self.last_used_phoneme: Optional[str] = None
        
    def get_first_phoneme(self, text: str) -> str:
        """éŸ³å£°èªè­˜çµæœã‹ã‚‰æœ€åˆã®éŸ³éŸ»ã‚’å–å¾—"""
        if not text:
            return ''
        return text[0]
    
    def get_random_words_from_phoneme(self, phoneme: str) -> List[str]:
        """éŸ³éŸ»ã‹ã‚‰å˜èªã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ"""
        if phoneme not in GOJUON_WORDS:
            return []
            
        word_list = GOJUON_WORDS[phoneme]
        selected_count = min(self.config.max_selected_words, len(word_list))
        selected_pairs = random.sample(word_list, selected_count)
        return [pair[1] for pair in selected_pairs]  # è‹±èªè¡¨ç¾ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
    
    def process_voice_input(self, text: str) -> Tuple[str, List[str]]:
        """éŸ³å£°å…¥åŠ›ã‚’å‡¦ç†ã—ã¦éŸ³éŸ»ã¨å˜èªã‚’è¿”ã™"""
        if not text:
            return "", []
            
        # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æœ€åˆã®2æ–‡å­—åˆ†ã®éŸ³éŸ»ã‚’å–å¾—
        first_two_phonemes = [char for char in text[:2] if char in GOJUON_WORDS]
        if not first_two_phonemes:
            return "", []
        
        # éŸ³éŸ»ã®é¸æŠï¼ˆå‰å›ã¨åŒã˜å ´åˆã¯2æ–‡å­—ç›®ã‚’ä½¿ç”¨ï¼‰
        selected_phoneme = first_two_phonemes[0]
        if (self.last_used_phoneme and 
            selected_phoneme == self.last_used_phoneme and 
            len(first_two_phonemes) > 1):
            selected_phoneme = first_two_phonemes[1]
        
        self.last_used_phoneme = selected_phoneme
        english_words = self.get_random_words_from_phoneme(selected_phoneme)
        
        return selected_phoneme, english_words

# éŸ³å£°èªè­˜ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
class VoiceRecognitionHandler:
    """éŸ³å£°èªè­˜å‡¦ç†"""
    
    def __init__(self, config: AppConfig, phoneme_processor: PhonemeProcessor):
        self.config = config
        self.phoneme_processor = phoneme_processor
        self.recognition_active = config.voice_recognition_enabled
        self.running = False
        self.voice_callback: Optional[Callable] = None
        
        if self.recognition_active:
            self._initialize_voice_recognition()
    
    def _initialize_voice_recognition(self):
        """éŸ³å£°èªè­˜ã®åˆæœŸåŒ–"""
        try:
            import speech_recognition as sr
            self.recognizer = sr.Recognizer()
            self.recognizer.dynamic_energy_threshold = True
            self.recognizer.energy_threshold = 4000
            print("ğŸ¤ éŸ³å£°èªè­˜ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
        except ImportError:
            print("âŒ speech_recognitionãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            self.recognition_active = False
        except Exception as e:
            print(f"âŒ éŸ³å£°èªè­˜ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
            self.recognition_active = False
    
    def set_voice_callback(self, callback: Callable[[str, List[str]], None]):
        """éŸ³å£°èªè­˜çµæœã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š"""
        self.voice_callback = callback
    
    def start_listening(self):
        """éŸ³å£°èªè­˜é–‹å§‹"""
        if not self.recognition_active:
            return
            
        self.running = True
        self.voice_thread = threading.Thread(target=self._listen_loop, daemon=True)
        self.voice_thread.start()
        print("ğŸ¤ éŸ³å£°èªè­˜ã‚’é–‹å§‹ã—ã¾ã—ãŸ")
    
    def stop_listening(self):
        """éŸ³å£°èªè­˜åœæ­¢"""
        self.running = False
    
    def _listen_loop(self):
        """éŸ³å£°èªè­˜ãƒ«ãƒ¼ãƒ—"""
        import speech_recognition as sr
        
        while self.running:
            try:
                with sr.Microphone() as source:
                    if not hasattr(self, 'first_adjustment'):
                        print("ğŸ”§ ãƒã‚¤ã‚¯ã®ãƒã‚¤ã‚ºèª¿æ•´ä¸­...")
                        self.recognizer.adjust_for_ambient_noise(source, duration=1)
                        self.first_adjustment = True
                    
                    print("ğŸ¤ éŸ³å£°èªè­˜å¾…æ©Ÿä¸­...")
                    audio = self.recognizer.listen(source, phrase_time_limit=3)
                    text = self.recognizer.recognize_google(audio, language='ja-JP')
                    print(f"ğŸ—£ï¸ èªè­˜ã•ã‚ŒãŸéŸ³å£°: {text}")
                    
                    # éŸ³éŸ»å‡¦ç†
                    phoneme, words = self.phoneme_processor.process_voice_input(text)
                    if words and self.voice_callback:
                        self.voice_callback(phoneme, words)
                        
            except sr.UnknownValueError:
                pass  # èªè­˜ã§ããªã„å ´åˆã¯ç„¡è¦–
            except sr.RequestError as e:
                print(f"âŒ éŸ³å£°èªè­˜ã‚µãƒ¼ãƒ“ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            except Exception as e:
                print(f"âŒ éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(1)

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚¯ãƒ©ã‚¹
class PromptManager:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç”Ÿæˆãƒ»ç®¡ç†ãƒ»å±¥æ­´"""
    
    def __init__(self, config: AppConfig):
        self.config = config
        self.current_prompt: str = ""
        self.prompt_history: List[str] = []
        self.active_prompts: List[str] = []
        self.last_update_time = time.time()
        
        # LLM APIè¨­å®š
        load_dotenv()
        self.llm_available = False
        
        # åˆæœŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šï¼ˆStreamDiffusionã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«åˆã‚ã›ã‚‹ï¼‰
        self.default_prompt = "moon"
        self.base_negative_prompt = ("human, person, people, face, portrait, "
                                   "low quality, bad quality, blurry, low resolution")
    
    def _check_llm_availability(self) -> bool:
        """LLM APIã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        return False
    
    def generate_random_prompt(self) -> str:
        """ãƒ©ãƒ³ãƒ€ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"""
        themes = [
            "moonlit landscape", "impressionist garden", "watercolor nature",
            "serene mountain view", "peaceful lakeside", "autumn forest",
            "gentle sunrise", "quiet village scene", "classical still life",
            "pastoral countryside", "misty morning", "soft floral arrangement"
        ]
        
        theme = random.choice(themes)
        
        # LLMãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯åŸºæœ¬çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿”ã™
        if not self.llm_available:
            return f"{theme}, impressionist painting style, soft brushstrokes, gentle lighting"
        
        return f"{theme}, impressionist painting style, soft brushstrokes, gentle lighting, serene atmosphere"
    
    def enhance_prompt(self, user_prompt: str) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¼·åŒ–"""
        if not user_prompt.strip():
            return self.generate_random_prompt()
        
        # Simple enhancement without LLM
        enhanced = f"{user_prompt}, impressionist painting style, soft colors, gentle lighting, serene atmosphere"
        return enhanced
    
    def add_phoneme_prompts(self, phoneme: str, words: List[str]):
        """éŸ³éŸ»ãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ """
        if not words:
            return
        
        # é‡è¤‡é™¤å»ã¨ä¸Šé™ç®¡ç†
        new_words = [w for w in words if w not in self.active_prompts]
        self.active_prompts.extend(new_words)
        
        if len(self.active_prompts) > self.config.phoneme_max_prompts:
            excess = len(self.active_prompts) - self.config.phoneme_max_prompts
            self.active_prompts = self.active_prompts[excess:]
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        combined_prompt = f"{self.default_prompt}, {', '.join(self.active_prompts)}"
        self.update_current_prompt(combined_prompt)
        
        print(f"ğŸµ éŸ³éŸ»ã€Œ{phoneme}ã€â†’ {', '.join(words)}")
        print(f"ğŸ“ ç¾åœ¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {combined_prompt}")
    
    def update_current_prompt(self, prompt: str):
        """ç¾åœ¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ›´æ–°"""
        self.current_prompt = prompt
        self.add_to_history(prompt)
        self.last_update_time = time.time()
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆ‡ã‚Šæ›¿ãˆã‚’ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã«å‡ºåŠ›
        print(f"ğŸ¨ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ›´æ–°: {prompt}")
    
    def add_to_history(self, prompt: str):
        """å±¥æ­´ã«è¿½åŠ """
        if len(self.prompt_history) >= self.config.max_history:
            self.prompt_history.pop(0)
        self.prompt_history.append(prompt)
    
    def can_update_prompt(self) -> bool:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ›´æ–°å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        return (time.time() - self.last_update_time) >= self.config.phoneme_update_interval

# ã‚«ãƒ¡ãƒ©ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã‚¯ãƒ©ã‚¹  
class CameraController:
    """ã‚«ãƒ¡ãƒ©åˆ¶å¾¡"""
    
    def __init__(self, config: AppConfig):
        self.config = config
        self.cap: Optional[cv2.VideoCapture] = None
        self.camera_id: Optional[int] = None
    
    def find_available_cameras(self) -> List[int]:
        """åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ¡ãƒ©ã‚’æ¤œå‡º"""
        available_cameras = []
        for i in range(10):
            cap = cv2.VideoCapture(i)
            if cap.isOpened():
                ret, _ = cap.read()
                if ret:
                    available_cameras.append(i)
                cap.release()
        return available_cameras
    
    def select_camera(self) -> Optional[int]:
        """ã‚«ãƒ¡ãƒ©é¸æŠ"""
        available_cameras = self.find_available_cameras()
        
        if not available_cameras:
            print("âŒ åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ¡ãƒ©ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return None
        
        print(f"ğŸ¥ åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ¡ãƒ©: {available_cameras}")
        
        if len(available_cameras) == 1:
            camera_id = available_cameras[0]
            print(f"ğŸ“¹ ã‚«ãƒ¡ãƒ© {camera_id} ã‚’ä½¿ç”¨ã—ã¾ã™")
            return camera_id
        
        while True:
            try:
                choice = input(f"ä½¿ç”¨ã™ã‚‹ã‚«ãƒ¡ãƒ©ã‚’é¸æŠã—ã¦ãã ã•ã„ {available_cameras}: ")
                camera_id = int(choice)
                if camera_id in available_cameras:
                    return camera_id
                print(f"âŒ ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚{available_cameras} ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„")
            except ValueError:
                print("âŒ æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    
    def initialize_camera(self) -> bool:
        """ã‚«ãƒ¡ãƒ©åˆæœŸåŒ–"""
        self.camera_id = self.select_camera()
        if self.camera_id is None:
            return False
        
        self.cap = cv2.VideoCapture(self.camera_id)
        if not self.cap.isOpened():
            print(f"âŒ ã‚«ãƒ¡ãƒ© {self.camera_id} ã‚’é–‹ã‘ã¾ã›ã‚“ã§ã—ãŸ")
            return False
        
        return True
    
    def read_frame(self) -> Optional[np.ndarray]:
        """ãƒ•ãƒ¬ãƒ¼ãƒ èª­ã¿å–ã‚Š"""
        if self.cap is None:
            return None
        
        ret, frame = self.cap.read()
        return frame if ret else None
    
    def release(self):
        """ã‚«ãƒ¡ãƒ©ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾"""
        if self.cap:
            self.cap.release()

# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚µãƒ¼ãƒãƒ¼ã‚¯ãƒ©ã‚¹
class NetworkServer:
    """TCPé€šä¿¡ã‚µãƒ¼ãƒãƒ¼"""
    
    def __init__(self, config: AppConfig):
        self.config = config
        self.running = False
        self.server_socket: Optional[socket.socket] = None
        self.prompt_callback: Optional[Callable] = None
        
    def set_prompt_callback(self, callback: Callable[[str], None]):
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå—ä¿¡ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š"""
        self.prompt_callback = callback
    
    def start_server(self):
        """ã‚µãƒ¼ãƒãƒ¼é–‹å§‹"""
        self.running = True
        self.server_thread = threading.Thread(target=self._server_loop, daemon=True)
        self.server_thread.start()
        print(f"ğŸŒ TCPã‚µãƒ¼ãƒãƒ¼ã‚’é–‹å§‹: {self.config.tcp_host}:{self.config.tcp_port}")
    
    def stop_server(self):
        """ã‚µãƒ¼ãƒãƒ¼åœæ­¢"""
        self.running = False
        if self.server_socket:
            self.server_socket.close()
    
    def _server_loop(self):
        """ã‚µãƒ¼ãƒãƒ¼ãƒ«ãƒ¼ãƒ—"""
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                s.bind((self.config.tcp_host, self.config.tcp_port))
                s.listen()
                self.server_socket = s
                
                while self.running:
                    try:
                        conn, addr = s.accept()
                        self._handle_client(conn, addr)
                    except Exception as e:
                        if self.running:
                            print(f"âŒ ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
                            
        except Exception as e:
            print(f"âŒ ã‚µãƒ¼ãƒãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _handle_client(self, conn: socket.socket, addr):
        """ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå‡¦ç†"""
        print(f"ğŸ”— ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶š: {addr}")
        try:
            with conn:
                while True:
                    data = conn.recv(1024)
                    if not data:
                        break
                    
                    prompt = data.decode('utf-8').strip()
                    print(f"ğŸ“¨ å—ä¿¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
                    
                    if self.prompt_callback:
                        self.prompt_callback(prompt)
                    
                    conn.sendall(f"âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå—ä¿¡: {prompt}".encode('utf-8'))
        except Exception as e:
            print(f"âŒ ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼
class UIController:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹åˆ¶å¾¡"""
    
    def __init__(self, config: AppConfig):
        self.config = config
        self.last_output_image: Optional[Image.Image] = None
        
        # ç”»åƒå±¥æ­´ç®¡ç†ï¼ˆè»½é‡åŒ–ã®ãŸã‚æœ€å¤§3æšã¾ã§ï¼‰
        self.image_history: List[Image.Image] = []
        self.history_size = 3
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ç®¡ç†
        self.window_name = "StreamDiffusion éŸ³éŸ»ãƒˆãƒªã‚¬ãƒ¼"
        self.window_initialized = False
        
    def show_instructions(self):
        """æ“ä½œèª¬æ˜è¡¨ç¤º"""
        print("\n" + "="*45)
        print("ğŸ¨ StreamDiffusion éŸ³éŸ»ãƒˆãƒªã‚¬ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ")
        print("="*45)
        print("ğŸ¤ ãƒã‚¤ã‚¯ã«å‘ã‹ã£ã¦æ—¥æœ¬èªã§è©±ã—ã¦ãã ã•ã„")
        print("ğŸµ æœ€åˆã®éŸ³éŸ»ã«å¯¾å¿œã—ãŸç”»åƒãŒç”Ÿæˆã•ã‚Œã¾ã™")
        print(f"ğŸŒ TCPæ¥ç¶š: {self.config.tcp_host}:{self.config.tcp_port}")
        print("\nâŒ¨ï¸  ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰æ“ä½œ:")
        print("[r] ãƒ©ãƒ³ãƒ€ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ")
        print("[i] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›ãƒ¢ãƒ¼ãƒ‰") 
        print("[s] ç¾åœ¨ã®ç”»åƒã‚’ä¿å­˜")
        print("[p] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå±¥æ­´è¡¨ç¤º")
        print("[c] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒªã‚»ãƒƒãƒˆï¼ˆmoonã®ã¿ï¼‰")
        print("[q] çµ‚äº†")
        print("="*45 + "\n")
    
    def display_frame(self, output_image: Image.Image):
        """ãƒ•ãƒ¬ãƒ¼ãƒ è¡¨ç¤ºï¼ˆå±¥æ­´ç®¡ç†ä»˜ããƒ»å…¨ç”»é¢è¡¨ç¤ºãƒ»ä¸­å¤®å¯„ã›ï¼‰"""
        # å±¥æ­´ã«è¿½åŠ 
        self._add_to_history(output_image)
        
        # é€æ˜åº¦ã§é‡ã­åˆã‚ã›ãŸç”»åƒã‚’ä½œæˆ
        blended_image = self._create_blended_image()
        
        # ä¸­å¤®å¯„ã›ç”¨ã®ç”»åƒã‚’ä½œæˆï¼ˆ16:9æ¯”ç‡ï¼‰
        centered_image = self._create_centered_display(blended_image)
        
        self.last_output_image = blended_image
        output_np = cv2.cvtColor(np.array(centered_image), cv2.COLOR_RGB2BGR)
        
        # åˆå›ã®ã¿ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ä½œæˆãƒ»è¨­å®š
        if not self.window_initialized:
            height, width = output_np.shape[:2]
            print(f"ğŸ” ãƒ‡ãƒãƒƒã‚° - ç”»åƒã‚µã‚¤ã‚º: {width}x{height}")
            
            cv2.namedWindow(self.window_name, cv2.WINDOW_NORMAL)
            # ç”»åƒã‚µã‚¤ã‚ºã«å¼·åˆ¶ãƒªã‚µã‚¤ã‚ºã—ã¦ã‹ã‚‰å…¨ç”»é¢åŒ–
            cv2.resizeWindow(self.window_name, width, height)
            cv2.setWindowProperty(self.window_name, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)
            self.window_initialized = True
        
        # ç”»åƒã®ã¿æ›´æ–°
        cv2.imshow(self.window_name, output_np)
    
    def _add_to_history(self, image: Image.Image):
        """ç”»åƒã‚’å±¥æ­´ã«è¿½åŠ """
        # æ–°ã—ã„ç”»åƒã‚’å…ˆé ­ã«è¿½åŠ 
        self.image_history.insert(0, image.copy())
        
        # å±¥æ­´ã‚µã‚¤ã‚ºã‚’åˆ¶é™
        if len(self.image_history) > self.history_size:
            self.image_history = self.image_history[:self.history_size]
    
    def _create_blended_image(self) -> Image.Image:
        """é€æ˜åº¦ã‚’ä½¿ã£ã¦ç”»åƒã‚’é‡ã­åˆã‚ã›"""
        if not self.image_history:
            # é»’ã„ç”»åƒã‚’è¿”ã™
            return Image.new('RGB', (512, 512), color=(0, 0, 0))
        
        # é€æ˜åº¦è¨­å®š
        alphas = [0.6, 0.2, 0.1]  # t, t-1, t-2ã®é€æ˜åº¦
        
        # ãƒ™ãƒ¼ã‚¹ç”»åƒï¼ˆæœ€æ–°ï¼‰
        result = self.image_history[0].copy().convert('RGBA')
        
        # å¤ã„ç”»åƒã‚’é‡ã­åˆã‚ã›
        for i in range(1, min(len(self.image_history), len(alphas))):
            alpha = alphas[i]
            old_image = self.image_history[i].convert('RGBA')
            
            # ã‚¢ãƒ«ãƒ•ã‚¡ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°
            overlay = Image.new('RGBA', result.size, (0, 0, 0, 0))
            overlay.paste(old_image, (0, 0))
            
            # é€æ˜åº¦ã‚’é©ç”¨
            overlay.putalpha(int(255 * alpha))
            
            # é‡ã­åˆã‚ã›
            result = Image.alpha_composite(result, overlay)
        
        # RGBã«å¤‰æ›ã—ã¦è¿”ã™
        return result.convert('RGB')
    
    def _create_centered_display(self, image: Image.Image) -> Image.Image:
        """æ­£æ–¹å½¢ç”»åƒã‚’ç”»é¢ä¸­å¤®ã«é…ç½®ï¼ˆä¸Šä¸‹å·¦å³é»’è‰²ï¼‰"""
        # M4 MacBook Proè§£åƒåº¦ã«å›ºå®š
        display_width = 2560
        display_height = 1664
        
        # æ­£æ–¹å½¢ç”»åƒã®ã‚µã‚¤ã‚ºï¼ˆç”»é¢ã®çŸ­è¾ºã«åˆã‚ã›ã‚‹ï¼‰
        square_size = min(display_width, display_height)  # 1664x1664
        
        # é»’ã„èƒŒæ™¯ã‚’ä½œæˆ
        display_image = Image.new('RGB', (display_width, display_height), color=(0, 0, 0))
        
        # æ­£æ–¹å½¢ç”»åƒã‚’ãƒªã‚µã‚¤ã‚º
        resized_image = image.resize((square_size, square_size), Image.LANCZOS)
        
        # ä¸­å¤®ã«é…ç½®ï¼ˆä¸Šä¸‹å·¦å³å®Œå…¨ä¸­å¤®ï¼‰
        x_offset = (display_width - square_size) // 2  # æ°´å¹³ä¸­å¤®
        y_offset = (display_height - square_size) // 2  # å‚ç›´ä¸­å¤®
        
        display_image.paste(resized_image, (x_offset, y_offset))
        
        return display_image
    
    def save_to_gallery(self, image: Image.Image, prompt: str) -> str:
        """ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã«ä¿å­˜"""
        os.makedirs(self.config.gallery_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{self.config.gallery_dir}/img_{timestamp}.png"
        
        image.save(filename)
        with open(f"{self.config.gallery_dir}/img_{timestamp}.json", "w") as f:
            json.dump({"prompt": prompt, "timestamp": timestamp}, f)
        
        return filename

# ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¯ãƒ©ã‚¹
class WebCameraApp:
    """ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    def __init__(self):
        self.config = AppConfig()
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.phoneme_processor = PhonemeProcessor(self.config)
        self.prompt_manager = PromptManager(self.config)
        self.camera_controller = CameraController(self.config)
        self.voice_handler = VoiceRecognitionHandler(self.config, self.phoneme_processor)
        self.network_server = NetworkServer(self.config)
        self.ui_controller = UIController(self.config)
        
        # StreamDiffusion
        self.stream_diffusion: Optional[StreamDiffusion] = None
        
        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
        self._setup_callbacks()
    
    def _setup_callbacks(self):
        """ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š"""
        self.voice_handler.set_voice_callback(self._on_voice_input)
        self.network_server.set_prompt_callback(self._on_network_prompt)
    
    def _on_voice_input(self, phoneme: str, words: List[str]):
        """éŸ³å£°å…¥åŠ›ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        if self.prompt_manager.can_update_prompt():
            self.prompt_manager.add_phoneme_prompts(phoneme, words)
            self._update_stream_diffusion()  # StreamDiffusionæ›´æ–°ã‚’è¿½åŠ 
    
    def _on_network_prompt(self, prompt: str):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        enhanced_prompt = self.prompt_manager.enhance_prompt(prompt)
        self.prompt_manager.update_current_prompt(enhanced_prompt)
        self._update_stream_diffusion()  # StreamDiffusionæ›´æ–°ã‚’è¿½åŠ 
    
    def initialize(self) -> bool:
        """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–"""
        print("ğŸš€ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–ä¸­...")
        
        # ã‚«ãƒ¡ãƒ©åˆæœŸåŒ–
        if not self.camera_controller.initialize_camera():
            print("âŒ ã‚«ãƒ¡ãƒ©åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        # StreamDiffusionåˆæœŸåŒ–
        initial_prompt = self.prompt_manager.default_prompt
        # éŸ³éŸ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒªã‚¹ãƒˆã‚’ã‚¯ãƒªã‚¢
        self.prompt_manager.active_prompts = []
        self.prompt_manager.update_current_prompt(initial_prompt)
        print(f"ğŸ¨ åˆæœŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š: {initial_prompt}")
        
        try:
            self.stream_diffusion = StreamDiffusion(
                prompt=initial_prompt,
                local_cache_dir=self.config.local_cache_dir,
                optimize_for_speed=self.config.optimize_for_speed,
                use_kohaku_model=self.config.use_kohaku_model
            )
            print("âœ… StreamDiffusionåˆæœŸåŒ–å®Œäº†ï¼ˆrefç‰ˆæœ€é©åŒ–é©ç”¨ï¼‰")
            
            # åˆæœŸåŒ–å¾Œã«æ˜ç¤ºçš„ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®š
            self._update_stream_diffusion()
        except Exception as e:
            print(f"âŒ StreamDiffusionåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
        
        # éŸ³å£°èªè­˜é–‹å§‹
        self.voice_handler.start_listening()
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚µãƒ¼ãƒãƒ¼é–‹å§‹
        self.network_server.start_server()
        
        return True
    
    def run(self):
        """ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œ"""
        if not self.initialize():
            return
        
        self.ui_controller.show_instructions()
        
        try:
            while True:
                # ãƒ•ãƒ¬ãƒ¼ãƒ å–å¾—
                frame = self.camera_controller.read_frame()
                if frame is None:
                    print("âŒ ã‚«ãƒ¡ãƒ©ã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    break
                
                # ç”»åƒå‡¦ç†
                init_img = self._preprocess_frame(frame)
                image_tensor = self.stream_diffusion.stream.preprocess_image(init_img)
                
                try:
                    output_image = self.stream_diffusion.stream(image_tensor)
                    if isinstance(output_image, Image.Image):
                        self.ui_controller.display_frame(output_image)
                except Exception as e:
                    print(f"âŒ ç”»åƒç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                    if self.ui_controller.last_output_image:
                        self.ui_controller.display_frame(self.ui_controller.last_output_image)
                
                # ã‚­ãƒ¼å…¥åŠ›å‡¦ç†
                if not self._handle_keyboard_input():
                    break
                    
        except KeyboardInterrupt:
            print("ğŸ‘‹ ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰å‰²ã‚Šè¾¼ã¿ã§çµ‚äº†")
        except Exception as e:
            print(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            self._cleanup()
    
    def _preprocess_frame(self, frame: np.ndarray) -> Image.Image:
        """ãƒ•ãƒ¬ãƒ¼ãƒ å‰å‡¦ç†"""
        pil_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        return self._crop_center(
            pil_frame, 
            self.config.sd_side_length * 2, 
            self.config.sd_side_length * 2
        ).resize((self.config.sd_side_length, self.config.sd_side_length), Image.NEAREST)
    
    def _crop_center(self, pil_img: Image.Image, crop_width: int, crop_height: int) -> Image.Image:
        """ä¸­å¤®ã‚¯ãƒ­ãƒƒãƒ—"""
        img_width, img_height = pil_img.size
        return pil_img.crop((
            (img_width - crop_width) // 2,
            (img_height - crop_height) // 2,
            (img_width + crop_width) // 2,
            (img_height + crop_height) // 2,
        ))
    
    def _handle_keyboard_input(self) -> bool:
        """ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰å…¥åŠ›å‡¦ç†"""
        key = cv2.waitKey(1) & 0xFF
        
        if key == ord('q'):
            print("ğŸ‘‹ çµ‚äº†ã—ã¾ã™")
            return False
        elif key == ord('r'):
            new_prompt = self.prompt_manager.generate_random_prompt()
            self.prompt_manager.update_current_prompt(new_prompt)
            self._update_stream_diffusion()
            print(f"ğŸ² ãƒ©ãƒ³ãƒ€ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {new_prompt}")
        elif key == ord('i'):
            self._interactive_prompt_input()
        elif key == ord('s') and self.ui_controller.last_output_image:
            filename = self.ui_controller.save_to_gallery(
                self.ui_controller.last_output_image, 
                self.prompt_manager.current_prompt
            )
            print(f"ğŸ’¾ ç”»åƒä¿å­˜: {filename}")
        elif key == ord('p'):
            self._show_prompt_history()
        elif key == ord('c'):
            self._reset_to_default_prompt()
        
        return True
    
    def _interactive_prompt_input(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›"""
        print("\nğŸ’¡ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›ãƒ¢ãƒ¼ãƒ‰")
        try:
            user_input = input("æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (ç©ºç™½ã§ãƒ©ãƒ³ãƒ€ãƒ ): ").strip()
            
            if user_input == "":
                new_prompt = self.prompt_manager.generate_random_prompt()
                print(f"ğŸ² ãƒ©ãƒ³ãƒ€ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {new_prompt}")
            else:
                new_prompt = self.prompt_manager.enhance_prompt(user_input)
                print(f"âœ¨ å¼·åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {new_prompt}")
            
            self.prompt_manager.update_current_prompt(new_prompt)
            self._update_stream_diffusion()
            
        except Exception as e:
            print(f"âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _show_prompt_history(self):
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå±¥æ­´è¡¨ç¤º"""
        print("\nğŸ“š ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå±¥æ­´:")
        for i, prompt in enumerate(self.prompt_manager.prompt_history, 1):
            print(f"{i}. {prompt}")
        print()
    
    def _reset_to_default_prompt(self):
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆmoonï¼‰ã«ãƒªã‚»ãƒƒãƒˆ"""
        self.prompt_manager.active_prompts = []
        self.prompt_manager.update_current_prompt(self.prompt_manager.default_prompt)
        self._update_stream_diffusion()
        print(f"ğŸ”„ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒªã‚»ãƒƒãƒˆ: {self.prompt_manager.default_prompt}")
    
    def _update_stream_diffusion(self):
        """StreamDiffusionæ›´æ–°"""
        try:
            self.stream_diffusion.stream.prepare(
                prompt=self.prompt_manager.current_prompt,
                negative_prompt=self.prompt_manager.base_negative_prompt,
                num_inference_steps=30,  # 50â†’30ã«æ¸›ã‚‰ã—ã¦é«˜é€ŸåŒ–
                guidance_scale=2.5,     # 1.2â†’2.5ã«ä¸Šã’ã¦ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ã«
                delta=0.3,              # 0.2â†’0.3ã«ä¸Šã’ã¦ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ã•ã¨åŸå‹ä¿æŒã®ãƒãƒ©ãƒ³ã‚¹
            )
        except Exception as e:
            print(f"âŒ StreamDiffusionæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾"""
        print("ğŸ§¹ ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾ä¸­...")
        self.voice_handler.stop_listening()
        self.network_server.stop_server()
        self.camera_controller.release()
        cv2.destroyAllWindows()
        print("âœ… çµ‚äº†å®Œäº†")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    app = WebCameraApp()
    app.run()

if __name__ == "__main__":
    main()